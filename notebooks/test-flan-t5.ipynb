{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/run/cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=100\n",
    "    )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_answer(question):\n",
    "    prompt = f\"\"\"Please solve the following high school math problem step by step. Explain your reasoning clearly and provide the final answer.\n",
    "\n",
    "{question}\n",
    "\n",
    "Step-by-step solution and final answer:\"\"\"\n",
    "\n",
    "    return llm(prompt)\n",
    "\n",
    "def parse_answer(question, message):\n",
    "    prompt = f\"\"\" Please analyze the solution for a given problem, fix it if needed, and then provide the final answer.         \n",
    "Your response should end in the format: 'Hence, the final answer is [numeric string].\n",
    "    \n",
    "Q: {question}\n",
    "\n",
    "Solution: {message}\n",
    "\n",
    "Analysis: \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return llm(prompt)\n",
    "\n",
    "\n",
    "def extract_numerical_answer(text):\n",
    "    # Look for patterns like \"Final answer: X\" or \"The answer is X\" at the end of the text\n",
    "    match = re.search(r'(?:final answer|the answer is)[:\\s]*([+-]?\\d*\\.?\\d+)', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    else:\n",
    "        # If no clear final answer, look for the last number in the text\n",
    "        numbers = re.findall(r'[+-]?\\d*\\.?\\d+', text)\n",
    "        return float(numbers[-1]) if numbers else 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = test_df.iloc[0]['problem_text']\n",
    "response_1 = get_answer(question)\n",
    "response_1 = response_1.strip()\n",
    "response_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_2 = parse_answer(question, response_1)\n",
    "response_2 = response_2.strip()\n",
    "response_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_answer = extract_numerical_answer(parse_answer(question, response_1))\n",
    "numerical_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "GLOBAL_SOLUTIONS_FOR_SUBMISSION = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "\n",
    "    question = row['problem_text']\n",
    "    response_1 = get_answer(question)\n",
    "    response_1 = response_1.strip()\n",
    "    \n",
    "    response_2 = parse_answer(question, response_1)\n",
    "    response_2 = response_2.strip()\n",
    "    \n",
    "    numerical_answer = extract_numerical_answer(response_2)\n",
    "    \n",
    "    GLOBAL_SOLUTIONS_FOR_SUBMISSION.append({\n",
    "        'problem_id': row['problem_id'],\n",
    "        'llm_out_1': response_1,\n",
    "        'llm_out_2': response_2,\n",
    "        'answer': numerical_answer\n",
    "    })\n",
    "    \n",
    "pd.DataFrame(GLOBAL_SOLUTIONS_FOR_SUBMISSION).to_csv('baseline_answers_with_double_check.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(GLOBAL_SOLUTIONS_FOR_SUBMISSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
